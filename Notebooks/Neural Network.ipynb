{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from Preprocessing import preprocessing\n",
    "from Preprocessing.single_set import SingleSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Features: data.data_features\n",
    "Targets: data.data_targets (click, bidprice, payprice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- data loaded --\n",
      "-- data loaded --\n",
      "-- data loaded --\n"
     ]
    }
   ],
   "source": [
    "train_data_path = '/Data/train.csv'\n",
    "train_data = SingleSet(relative_path=train_data_path,use_numerical_labels=True)\n",
    "\n",
    "val_data_path = '/Data/validation.csv'\n",
    "val_data = SingleSet(relative_path=val_data_path,use_numerical_labels=True)\n",
    "\n",
    "test_data_path = '/Data/test.csv'\n",
    "test_data = SingleSet(relative_path=test_data_path,use_numerical_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_to_numpy(data):\n",
    "\n",
    "    ## features\n",
    "    features = np.asarray(data.data_features.values)\n",
    "\n",
    "    ## targets\n",
    "    if hasattr(data, \"data_targets\"):\n",
    "        labels = np.asarray(data.data_targets.values)\n",
    "        \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "## drop unnecessary features\n",
    "def drop_features(data):\n",
    "    \n",
    "    if 'userid' in data.data_features:\n",
    "        data.data_features.drop('userid', axis=1, inplace = True)\n",
    "        \n",
    "    if 'urlid' in data.data_features:\n",
    "        data.data_features.drop('bidid', axis=1, inplace = True)\n",
    "\n",
    "\n",
    "drop_features(train_data)\n",
    "drop_features(val_data)\n",
    "drop_features(test_data)\n",
    "\n",
    "x_train, y_train = pandas_to_numpy(train_data)\n",
    "x_val, y_val = pandas_to_numpy(val_data)\n",
    "x_test, y_test = pandas_to_numpy(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Input Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape 20\n"
     ]
    }
   ],
   "source": [
    "input_shape = x_train.shape[1]\n",
    "print(\"input_shape\", input_shape)\n",
    "output_shape = 1\n",
    "\n",
    "# targets_________________________________________________\n",
    "\n",
    "# clicks\n",
    "y_train_clicks = np.reshape(y_train[:,0], (y_train.shape[0], 1))  # get first column (clicks)\n",
    "y_val_clicks = np.reshape(y_val[:,0], (y_val.shape[0], 1))  # get first column (clicks)\n",
    "\n",
    "# payprice\n",
    "y_train_payprice = np.reshape(y_train[:,2], (y_train.shape[0], 1))  # get third column (payprice)\n",
    "y_val_payprice = np.reshape(y_val[:,2], (y_val.shape[0], 1))  # get third column (payprice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data attributes\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#normalized_X = preprocessing.normalize(x_train)\n",
    "\n",
    "\n",
    "\n",
    "## features\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "feature_scaler.fit(np.concatenate((x_train, x_val, x_test), axis = 0))       \n",
    "\n",
    "x_train = feature_scaler.transform(x_train)\n",
    "x_val = feature_scaler.transform(x_val)\n",
    "x_test = feature_scaler.transform(x_test)\n",
    "\n",
    "## Targets________________________________________\n",
    "\n",
    "# payprice\n",
    "#payprice_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#payprice_scaler.fit(np.concatenate((y_train_payprice, y_val_payprice), axis = 0))   \n",
    "\n",
    "#y_train_payprice = payprice_scaler.transform(y_train_payprice)\n",
    "#y_val_payprice = payprice_scaler.transform(y_val_payprice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Neural Networks\n",
    "\n",
    "## \"Click\" - Binary Classification\n",
    "\n",
    "Train Baseline Accuracy \"Clicks\": 0.9992618932746251%\n",
    "#of 0:    2429188     # of 1:       1793\n",
    "\n",
    "\n",
    "Val Baseline Accuracy \"Clicks\": 0.9993349203056733%\n",
    "#of 0:    303723     # of 1:       202\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "\n",
    "sample up \"1\"s for more balanced classification\n",
    "\n",
    "--> default accuracy: 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4858376, 19)\n"
     ]
    }
   ],
   "source": [
    "def upsampling(x, y):\n",
    "\n",
    "    xy = np.concatenate((x, y), axis = 1)\n",
    "\n",
    "    zeros = xy[xy[:,-1] == 0]\n",
    "    ones = xy[xy[:,-1] == 1]\n",
    "\n",
    "    ones_upsampled = np.repeat(ones, math.ceil(len(zeros)/len(ones)), axis=0)\n",
    "\n",
    "    # cut at length of zeros.shape 2429188\n",
    "    ones_upsampled = ones_upsampled[:zeros.shape[0]]\n",
    "\n",
    "    xy_upsampled  = np.concatenate((ones_upsampled, zeros), axis = 0) # combine\n",
    "    np.random.shuffle(xy_upsampled)                                   # shuffle\n",
    "\n",
    "    x_upsampled = xy_upsampled[:,:-1]   # features\n",
    "    y_upsampled = xy_upsampled[:,-1:]   # targets\n",
    "    \n",
    "    return x_upsampled, y_upsampled\n",
    "\n",
    "\n",
    "x_train_up, y_train_clicks_up = upsampling(x_train, y_train_clicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer / Categorical One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_clicks_up = keras.utils.to_categorical(y_train_clicks_up, 2)\n",
    "#y_val_clicks = keras.utils.to_categorical(y_val_clicks, 2)\n",
    "\n",
    "y_train_clicks_up = y_train_clicks_up.astype(int)\n",
    "y_val_clicks = y_val_clicks.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers, initializers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.models import Model, Input\n",
    "from keras import optimizers\n",
    "\n",
    "from random import shuffle\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"2\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', '.*do not.*',)\n",
    "\n",
    "\n",
    "\n",
    "def create_model_clicks(input_shape, output_shape):\n",
    "\n",
    "    ## sequential model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=input_shape, activation='relu'))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Dense(24, activation='relu'))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "\n",
    "    #model.add(Dropout(0.2))\n",
    "    #model.add(Dense(6, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    # working model___________\n",
    "    #model.add(Dense(3, input_dim=input_shape, activation='relu'))\n",
    "    #model.add(Dense(12, activation='relu'))\n",
    "    #model.add(Dense(12, activation='relu'))\n",
    "\n",
    "\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(output_shape, activation='sigmoid'))\n",
    "\n",
    "    ## other way of defining model\n",
    "    \n",
    "    #inputs = Input(shape=(input_shape,))\n",
    "    #outputs = Dense(output_shape, activation=\"relu\", kernel_regularizer=regularizers.l2(5e-4),kernel_initializer=initializers.he_normal(seed=13))(inputs)\n",
    "    #model = Model(inputs=inputs, outputs=outputs, name='bidder')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_curves_clicks(history):\n",
    "\n",
    "    # Loss Curves\n",
    "    plt.figure(figsize=[8, 6])\n",
    "    plt.plot(history.history['loss'],'cadetblue', linewidth=3.0)\n",
    "    plt.plot(history.history['val_loss'],'midnightblue', linewidth=3.0)\n",
    "    plt.legend(['Training loss', 'Validation Loss'], fontsize=12)\n",
    "    plt.xlabel('Epochs ', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Loss Curves', fontsize=16, fontweight=\"bold\")\n",
    "    #plt.savefig('Results/neural_network/loss_curve.png')\n",
    "    #plt.show()\n",
    "\n",
    "    # Accuracy Curves\n",
    "    plt.figure(figsize=[8, 6])\n",
    "    plt.plot(history.history['acc'],'cadetblue', linewidth=3.0)\n",
    "    plt.plot(history.history['val_acc'], 'midnightblue', linewidth=3.0)\n",
    "    plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=12)\n",
    "    plt.xlabel('Epochs ', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.title('Accuracy Curves', fontsize=16, fontweight=\"bold\")\n",
    "    #plt.savefig('Results/neural_network/accuracy_curve.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click Classifier - features:  20 targets:  1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 24)                504       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 1,129\n",
      "Trainable params: 1,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_31_input to have shape (20,) but got array with shape (19,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-cb8764376967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m history = model_clicks.fit(x=x_train_up, y=y_train_clicks_up, batch_size=batch_size, class_weight = class_weight,\n\u001b[0;32m---> 32\u001b[0;31m                     epochs=epochs, validation_data=(x_val, y_val_clicks), shuffle=True, callbacks=callbacks, verbose=1)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m## model evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_31_input to have shape (20,) but got array with shape (19,)"
     ]
    }
   ],
   "source": [
    "print(\"Click Classifier - features: \", input_shape, \"targets: \", output_shape)\n",
    "\n",
    "# Clear model, and create it\n",
    "model_clicks = create_model_clicks(input_shape, output_shape)\n",
    "print(model_clicks.summary())\n",
    "    \n",
    "\n",
    "## hyperparameters_______________________________________\n",
    "    \n",
    "\n",
    "## model compile and train\n",
    "model_clicks.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# specify learning rate\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "#model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "             ModelCheckpoint('Results/neural_network/' + 'trained_weights.h5', monitor='loss', save_best_only=True)]\n",
    "\n",
    "\n",
    "# Unbalanced Model weights\n",
    "class_weight = {0: 0.5, 1: 0.5} #{0: (1793 / 2429188), 1: (1-(1793 / 2429188))}\n",
    "\n",
    "# Fit the model on the batches\n",
    "batch_size = 16\n",
    "epochs = 16\n",
    "history = model_clicks.fit(x=x_train_up, y=y_train_clicks_up, batch_size=batch_size, class_weight = class_weight,\n",
    "                    epochs=epochs, validation_data=(x_val, y_val_clicks), shuffle=True, callbacks=callbacks, verbose=1)\n",
    "\n",
    "## model evaluate\n",
    "plot_curves_clicks(history)\n",
    "\n",
    "\n",
    "## save model to disk_____________________________________\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model_clicks.to_json()\n",
    "\n",
    "if not os.path.exists('Results/neural_network'):\n",
    "    os.makedirs('Results/neural_network/')\n",
    "\n",
    "with open('Results/neural_network/click_architecture.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model_clicks.save_weights('Results/neural_network/click_weights.h5')\n",
    "print(\"saved model to disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303925/303925 [==============================] - 2s 5us/step\n",
      "0    237725\n",
      "1     66200\n",
      "Name: click, dtype: int64\n",
      "\n",
      "F1_score: 0.877083388136186\n",
      "Accuracy: 0.7820580735378794\n"
     ]
    }
   ],
   "source": [
    "## Load Model for Testing_______________________________________________________________\n",
    "\n",
    "# Model reconstruction from JSON file\n",
    "#with open('Results/neural_network/' + 'model_architecture.json', 'r') as f:\n",
    "#    model_test = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "#model_test.load_weights('Results/neural_network/' + 'trained_weights.h5')\n",
    "\n",
    "#model_test.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "click_predictions = model_clicks.predict_classes(x_val, verbose=1)\n",
    "click_predictions_df = pd.DataFrame(click_predictions, columns= ['click'])\n",
    "\n",
    "print(click_predictions_df['click'].value_counts())\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"\\nF1_score:\", metrics.f1_score(y_val_clicks, click_predictions, average='weighted'))\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_val_clicks, click_predictions))\n",
    "\n",
    "\n",
    "#test_score = model.evaluate(x = x_val, y = y_val)\n",
    "#print('\\ntest accuracy:', test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Payprice\" - Float Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model \"Payprice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers, initializers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.models import Model, Input\n",
    "from keras import optimizers\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from random import shuffle\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"2\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', '.*do not.*',)\n",
    "\n",
    "\n",
    "\n",
    "def create_model_payprice(input_shape, output_shape):\n",
    "\n",
    "    ## sequential model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=input_shape, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    #model.add(Dense(12, input_dim=input_shape, activation='relu'))\n",
    "\n",
    "    # working model___________\n",
    "    #model.add(Dense(3, input_dim=input_shape, activation='relu'))\n",
    "    #model.add(Dense(12, activation='relu'))\n",
    "    #model.add(Dense(12, activation='relu'))\n",
    "\n",
    "\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(output_shape, activation='linear'))\n",
    "\n",
    "    ## other way of defining model\n",
    "    \n",
    "    #inputs = Input(shape=(input_shape,))\n",
    "    #outputs = Dense(output_shape, activation=\"relu\", kernel_regularizer=regularizers.l2(5e-4),kernel_initializer=initializers.he_normal(seed=13))(inputs)\n",
    "    #model = Model(inputs=inputs, outputs=outputs, name='bidder')\n",
    "\n",
    "    return model\n",
    "\n",
    "    \n",
    "def plot_curves(history):\n",
    "    \n",
    "    # Loss Curves\n",
    "    plt.figure(figsize=[8, 6])\n",
    "    plt.plot(history.history['loss'],'cadetblue', linewidth=3.0)\n",
    "    plt.plot(history.history['val_loss'],'midnightblue', linewidth=3.0)\n",
    "    plt.legend(['Training loss', 'Validation Loss'], fontsize=12)\n",
    "    plt.xlabel('Epochs ', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Loss Curves', fontsize=16, fontweight=\"bold\")\n",
    "    #plt.savefig('Results/neural_network/loss_curve.png')\n",
    "    #plt.show()\n",
    "\n",
    "    # Accuracy Curves\n",
    "    plt.figure(figsize=[8, 6])\n",
    "    plt.plot(history.history['mean_squared_error'],'cadetblue', linewidth=3.0)\n",
    "    plt.plot(history.history['val_mean_squared_error'], 'midnightblue', linewidth=3.0)\n",
    "    plt.legend(['Training MSE', 'Validation MSE'], fontsize=12)\n",
    "    plt.xlabel('Epochs ', fontsize=12)\n",
    "    plt.ylabel('Mean Squared Error', fontsize=12)\n",
    "    plt.title('Mean Squared Error Curves', fontsize=16, fontweight=\"bold\")\n",
    "    #plt.savefig('Results/neural_network/accuracy_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  20 targets:  1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 24)                504       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 1,129\n",
      "Trainable params: 1,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2430981 samples, validate on 303925 samples\n",
      "Epoch 1/16\n",
      "2430981/2430981 [==============================] - 143s 59us/step - loss: 1083880.5379 - mean_squared_error: 1083880.5379 - val_loss: 4290.6224 - val_mean_squared_error: 4290.6224\n",
      "Epoch 2/16\n",
      "2430981/2430981 [==============================] - 143s 59us/step - loss: 3198.6904 - mean_squared_error: 3198.6904 - val_loss: 3792.1012 - val_mean_squared_error: 3792.1012\n",
      "Epoch 3/16\n",
      "2430981/2430981 [==============================] - 143s 59us/step - loss: 3157.6285 - mean_squared_error: 3157.6285 - val_loss: 3788.4218 - val_mean_squared_error: 3788.4218\n",
      "Epoch 4/16\n",
      "2430981/2430981 [==============================] - 144s 59us/step - loss: 3136.4220 - mean_squared_error: 3136.4220 - val_loss: 3800.9092 - val_mean_squared_error: 3800.9092\n",
      "Epoch 5/16\n",
      "2430981/2430981 [==============================] - 144s 59us/step - loss: 3110.9219 - mean_squared_error: 3110.9219 - val_loss: 3815.6287 - val_mean_squared_error: 3815.6287\n",
      "Epoch 6/16\n",
      "2430981/2430981 [==============================] - 145s 60us/step - loss: 3095.6387 - mean_squared_error: 3095.6387 - val_loss: 3640.3377 - val_mean_squared_error: 3640.3377\n",
      "Epoch 7/16\n",
      "2430981/2430981 [==============================] - 145s 60us/step - loss: 3088.5856 - mean_squared_error: 3088.5856 - val_loss: 3673.3391 - val_mean_squared_error: 3673.3391\n",
      "Epoch 8/16\n",
      "2430981/2430981 [==============================] - 144s 59us/step - loss: 3070.8322 - mean_squared_error: 3070.8322 - val_loss: 3599.9555 - val_mean_squared_error: 3599.9555\n",
      "Epoch 9/16\n",
      "2430981/2430981 [==============================] - 144s 59us/step - loss: 3061.6890 - mean_squared_error: 3061.6890 - val_loss: 3574.5939 - val_mean_squared_error: 3574.5939\n",
      "Epoch 10/16\n",
      "2430981/2430981 [==============================] - 144s 59us/step - loss: 3050.6622 - mean_squared_error: 3050.6622 - val_loss: 3565.4016 - val_mean_squared_error: 3565.4016\n",
      "Epoch 11/16\n",
      "2430981/2430981 [==============================] - 146s 60us/step - loss: 3043.2428 - mean_squared_error: 3043.2428 - val_loss: 3425.3956 - val_mean_squared_error: 3425.3956\n",
      "Epoch 12/16\n",
      "2430981/2430981 [==============================] - 145s 60us/step - loss: 3037.2809 - mean_squared_error: 3037.2809 - val_loss: 3353.9442 - val_mean_squared_error: 3353.9442\n",
      "Epoch 13/16\n",
      "2430981/2430981 [==============================] - 146s 60us/step - loss: 3034.4763 - mean_squared_error: 3034.4763 - val_loss: 3348.1178 - val_mean_squared_error: 3348.1178\n",
      "Epoch 14/16\n",
      "1469216/2430981 [=================>............] - ETA: 55s - loss: 3026.6164 - mean_squared_error: 3026.6164"
     ]
    }
   ],
   "source": [
    "print(\"features: \", input_shape, \"targets: \", output_shape)\n",
    "\n",
    "# Clear model, and create it\n",
    "model_payprice = create_model_payprice(input_shape, output_shape)\n",
    "print(model_payprice.summary())\n",
    "\n",
    "\n",
    "## hyperparameters_______________________________________\n",
    "    \n",
    "\n",
    "## model compile and train\n",
    "model_payprice.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# specify learning rate\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "#model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "             ModelCheckpoint('Results/neural_network/' + 'trained_weights.h5', monitor='loss', save_best_only=True)]\n",
    "\n",
    "# Fit the model on the batches\n",
    "batch_size = 16\n",
    "epochs = 16\n",
    "history = model_payprice.fit(x=x_train, y=y_train_payprice, batch_size=batch_size, \n",
    "                    epochs=epochs, validation_data=(x_val, y_val_payprice), shuffle=True, callbacks=callbacks)\n",
    "# verbose=1, shuffle=True, callbacks=callbacks\n",
    "\n",
    "## model evaluate\n",
    "plot_curves(history)\n",
    "\n",
    "\n",
    "## save model to disk_____________________________________\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model_payprice.to_json()\n",
    "\n",
    "if not os.path.exists('Results/neural_network'):\n",
    "    os.makedirs('Results/neural_network/')\n",
    "\n",
    "with open('Results/neural_network/payprice_architecture.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model_payprice.save_weights('Results/neural_network/payprice_weights.h5')\n",
    "print(\"saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303925/303925 [==============================] - 2s 8us/step\n",
      "\n",
      "Mean Squared Error: 3297.598304984801\n",
      "\n",
      "           payprice\n",
      "0        65.272087\n",
      "1        63.293770\n",
      "2        64.296890\n",
      "3        64.017441\n",
      "4        63.370773\n",
      "5        58.483658\n",
      "6        72.513359\n",
      "7        77.601212\n",
      "8        64.276192\n",
      "9        58.483658\n",
      "10       91.474754\n",
      "11       58.804523\n",
      "12       58.705933\n",
      "13       63.113182\n",
      "14      134.359344\n",
      "15       62.680466\n",
      "16       63.365250\n",
      "17       61.052784\n",
      "18       59.031422\n",
      "19       58.770767\n",
      "20       58.483658\n",
      "21       63.530148\n",
      "22       63.306828\n",
      "23       64.190689\n",
      "24       58.483658\n",
      "25       61.681015\n",
      "26       81.997787\n",
      "27       59.908932\n",
      "28       58.483658\n",
      "29       83.857376\n",
      "...            ...\n",
      "303895  141.173004\n",
      "303896   58.483658\n",
      "303897   58.483658\n",
      "303898   63.297092\n",
      "303899   62.879070\n",
      "303900   58.483658\n",
      "303901   62.643410\n",
      "303902   60.305492\n",
      "303903   58.809887\n",
      "303904   58.483658\n",
      "303905   58.483658\n",
      "303906  103.571640\n",
      "303907   62.748775\n",
      "303908   84.069595\n",
      "303909   60.344345\n",
      "303910   81.940742\n",
      "303911   64.722328\n",
      "303912   81.983383\n",
      "303913   58.483658\n",
      "303914   62.578373\n",
      "303915   63.764839\n",
      "303916   58.483658\n",
      "303917   63.079266\n",
      "303918   58.483658\n",
      "303919   63.490536\n",
      "303920   59.341751\n",
      "303921   61.351730\n",
      "303922   62.509224\n",
      "303923   66.562447\n",
      "303924   64.118729\n",
      "\n",
      "[303925 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "## Load Model for Testing_______________________________________________________________\n",
    "\n",
    "# Model reconstruction from JSON file\n",
    "#with open('Results/neural_network/' + 'model_architecture.json', 'r') as f:\n",
    "#    model_test = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "#model_test.load_weights('Results/neural_network/' + 'trained_weights.h5')\n",
    "\n",
    "#model_test.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "payprice_predictions_scaled = model_payprice.predict(x_val, verbose=1)\n",
    "\n",
    "# scale back\n",
    "#payprice_predictions = payprice_scaler.inverse_transform(payprice_predictions_scaled)\n",
    "payprice_predictions_df = pd.DataFrame(payprice_predictions_scaled, columns= ['payprice'])\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"\\nMean Squared Error:\", metrics.mean_squared_error(y_val_payprice, payprice_predictions_scaled))\n",
    "print(\"\\n\", payprice_predictions_df)\n",
    "\n",
    "#test_score = model.evaluate(x = x_val, y = y_val)\n",
    "#print('\\ntest accuracy:', test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Bidding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Clicks and Payprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- data loaded --\n",
      "-- data loaded --\n",
      "-- data loaded --\n"
     ]
    }
   ],
   "source": [
    "train_data_path = '/Data/train.csv'\n",
    "train_data = SingleSet(relative_path=train_data_path,use_numerical_labels=True)\n",
    "\n",
    "val_data_path = '/Data/validation.csv'\n",
    "val_data = SingleSet(relative_path=val_data_path,use_numerical_labels=True)\n",
    "\n",
    "test_data_path = '/Data/test.csv'\n",
    "test_data = SingleSet(relative_path=test_data_path,use_numerical_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303375, 20)\n"
     ]
    }
   ],
   "source": [
    "def pandas_to_numpy(data):\n",
    "\n",
    "    ## features\n",
    "    features = np.asarray(data.data_features.values)\n",
    "\n",
    "    ## targets\n",
    "    if hasattr(data, \"data_targets\"):\n",
    "        labels = np.asarray(data.data_targets.values)\n",
    "        \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "\n",
    "## drop unnecessary features\n",
    "def drop_features(data):\n",
    "    \n",
    "    if 'userid' in data.data_features:\n",
    "        data.data_features.drop('userid', axis=1, inplace = True)\n",
    "        \n",
    "    if 'urlid' in data.data_features:\n",
    "        data.data_features.drop('urlid', axis=1, inplace = True)\n",
    "\n",
    "\n",
    "drop_features(train_data)\n",
    "drop_features(val_data)\n",
    "drop_features(test_data)\n",
    "\n",
    "x_train, y_train = pandas_to_numpy(train_data)\n",
    "x_val, y_val = pandas_to_numpy(val_data)\n",
    "x_test, y_test = pandas_to_numpy(test_data)\n",
    "\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data attributes\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## features\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "feature_scaler.fit(np.concatenate((x_train, x_val, x_test), axis = 0))       \n",
    "\n",
    "x_train = feature_scaler.transform(x_train)\n",
    "x_val = feature_scaler.transform(x_val)\n",
    "x_test = feature_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Models for Bidding _______________________________________________________________\n",
    "\n",
    "\n",
    "## CLICKS___________\n",
    "\n",
    "# Model reconstruction from JSON file\n",
    "with open('Results/neural_network/click_architecture.json', 'r') as f:\n",
    "    model_clicks = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model_clicks.load_weights('Results/neural_network/click_weights.h5')\n",
    "\n",
    "\n",
    "## PAYPRICE___________\n",
    "\n",
    "# Model reconstruction from JSON file\n",
    "with open('Results/neural_network/payprice_architecture.json', 'r') as f:\n",
    "    model_payprice = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model_payprice.load_weights('Results/neural_network/payprice_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303925/303925 [==============================] - 3s 8us/step\n",
      "303925/303925 [==============================] - 2s 8us/step\n"
     ]
    }
   ],
   "source": [
    "predict_data = x_val\n",
    "\n",
    "## CLICKS___________\n",
    "\n",
    "click_predictions = model_clicks.predict_classes(predict_data, verbose=1)\n",
    "\n",
    "## PAYPRICE___________\n",
    "\n",
    "payprice_predictions = model_payprice.predict(predict_data, verbose=1)\n",
    "#payprice_predictions = payprice_scaler.inverse_transform(payprice_predictions_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Bidding Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bids(bids, click_predictions, payprice_predictions):\n",
    "\n",
    "## 1.) Only bid for expected clicks!\n",
    "\n",
    "    for p in range(0, len(bids)):\n",
    "\n",
    "        if click_predictions[p] == 1:\n",
    "            #bids[p] = math.floor(payprice_predictions[p])\n",
    "            bids[p] = 71\n",
    "            \n",
    "    return bids\n",
    "\n",
    "\n",
    "\n",
    "def set_bidprices(bids, budget, payprice_predictions):\n",
    "    \n",
    "    ## 2.) Prefer cheap payprice predictions\n",
    "    \n",
    "    budget = 6250000\n",
    "    planned_bid_amount = sum(bids)\n",
    "    exceed_budget = 100\n",
    "    n_bids = len(bids[np.where(bids > 0)])\n",
    "\n",
    "    \n",
    "\n",
    "    ## (1) spend too much_______________________\n",
    "    \n",
    "    if planned_bid_amount - budget > 0:\n",
    "        \n",
    "        print(\"-- spend too much:\", planned_bid_amount - budget)\n",
    "        \n",
    "        while (planned_bid_amount - budget > exceed_budget):\n",
    "\n",
    "            #print(round(np.mean(bids)))\n",
    "            #print(budget - planned_bid_amount)\n",
    "            index, = list(np.where(bids == max(bids)))    # find expensive bids\n",
    "            bids[index] = max(bids) - 1                   # set expensive bid lower\n",
    "            planned_bid_amount = sum(bids)                # check new bidding amount\n",
    "\n",
    "    \n",
    "    ## (2) spend too little______________________\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(\"-- spend too little:\", budget - planned_bid_amount)\n",
    "        \n",
    "        while (budget - planned_bid_amount >  (-exceed_budget)):\n",
    "\n",
    "            #print(round(np.mean(bids)))\n",
    "            #print(budget - planned_bid_amount)\n",
    "            index, = list(np.where(bids == 0)) \n",
    "            index = random.sample(list(index), 1000)\n",
    "            bids[index] = 60              \n",
    "            planned_bid_amount = sum(bids)                \n",
    "\n",
    "\n",
    "    n_bids = len(bids[np.where(bids > 0)])  \n",
    "    print(\"planned_bid_amount:\", sum(bids), \", difference to budget:\", (budget - sum(bids)), \n",
    "              \", number of bids:\", n_bids, \", average bidprice:\",round(np.mean(bids[np.where(bids > 0)])))\n",
    "    \n",
    "    return bids\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- spend too little: 3454659.0\n",
      "planned_bid_amount: 6275341.0 , difference to budget: -25341.0 , number of bids: 97371 , average bidprice: 64.0\n"
     ]
    }
   ],
   "source": [
    "budget = 6250000\n",
    "bid_array = np.zeros((len(predict_data)))\n",
    "\n",
    "bid_decisions = set_bids(bid_array, click_predictions, payprice_predictions)\n",
    "bids = set_bidprices(bid_decisions, budget, payprice_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Decision in Auction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.abspath(os.pardir + '/Data/validation.csv')\n",
    "df = pd.read_csv(data_path, na_values=['Na', 'null']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bid# 0 , budget: 6250000 , payprice: 23 , bids_won: 0 , earned_clicks: 0 \n",
      "\n",
      "bid# 100000 , budget: 5695728 , payprice: 63 , bids_won: 16235 , earned_clicks: 12 \n",
      "\n",
      "bid# 200000 , budget: 5145324 , payprice: 196 , bids_won: 32468 , earned_clicks: 17 \n",
      "\n",
      "bid# 300000 , budget: 4593496 , payprice: 60 , bids_won: 48665 , earned_clicks: 30 \n",
      "\n",
      "__________________________________\n",
      "\n",
      "left budget: 4573175\n",
      "bids_won: 49262\n",
      "earned clicks: 30\n",
      "CTR: 0.0006089886728106857\n",
      "cost per click: 55894.166666666664\n"
     ]
    }
   ],
   "source": [
    "budget = 6250000\n",
    "\n",
    "## Evaluation Stats_____________\n",
    "\n",
    "bids_won = 0\n",
    "earned_clicks = 0\n",
    "ctr = 0                  # bids_won / earned_clicks\n",
    "total_paid = 0\n",
    "cpc = 0                  # cost per click\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    if bids[index] > budget: # check if budget is sufficient for bidprice\n",
    "        bids[index] = budget\n",
    "        #print(\"constant bid reduced to:\", constant_bid, \", total_paid:\", total_paid, \", bids_won:\", bids_won, \", earned clicks:\", earned_clicks, \"\\n\")\n",
    "\n",
    "    if budget <= 0:\n",
    "        print(\"-- break after auction #\", index)\n",
    "        break\n",
    "\n",
    "    # WON BID ______________________________________________\n",
    "\n",
    "    if bids[index] >= row['payprice']:     \n",
    "\n",
    "        bids_won += 1                        # won the bid\n",
    "        total_paid += row['payprice']        # add amount to total_paid   \n",
    "        budget = budget - row['payprice']    # substract money from budget\n",
    "\n",
    "        #if constant_bid == row['bidprice']:      \n",
    "            #budget = budget - row['payprice']    # substract money from budget\n",
    "\n",
    "        #elif constant_bid > row['bidprice']:\n",
    "        #    budget = budget - row['bidprice']    # substract money from budget\n",
    "\n",
    "        # CLICK = 1 ______________________________________________\n",
    "\n",
    "        if row['click'] == 1:    # only reduce money from budget if ad has been clicked\n",
    "\n",
    "                earned_clicks += 1                   # earn the click\n",
    "                #print(\"current budget:\", budget, \", earned clicks:\", earned_clicks, \"\\n\")\n",
    "\n",
    "    if index%100000 == 0:\n",
    "        print(\"bid#\", index, \", budget:\", budget, \", payprice:\", row['payprice'], \", bids_won:\", bids_won, \", earned_clicks:\", earned_clicks, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"__________________________________\\n\")\n",
    "\n",
    "if earned_clicks > 0:\n",
    "    cpc = total_paid / earned_clicks\n",
    "if bids_won > 0:\n",
    "    ctr = earned_clicks / bids_won\n",
    "\n",
    "print(\"left budget:\", budget)\n",
    "print(\"bids_won:\", bids_won)\n",
    "print(\"earned clicks:\", earned_clicks)\n",
    "print(\"CTR:\", ctr)\n",
    "print(\"cost per click:\", cpc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_test = os.path.abspath(os.pardir + '/Data/test.csv')\n",
    "df_test = pd.read_csv(data_path_test, na_values=['Na', 'null']).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidprice_series = pd.Series(data = bids, name='bidprice')\n",
    "submission_df = pd.DataFrame({'bidid': df_test['bidid'],'bidprice':bidprice_series})\n",
    "\n",
    "# Group Token: QQri5ISZz4Kn\n",
    "submission_df.to_csv('testing_bidding_price.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
